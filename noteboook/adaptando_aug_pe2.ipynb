{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyzeG-eoBMS5"
      },
      "source": [
        "# INSTALLLLS\n",
        "Instalaciones antes de arrancar, si esta cerca de morirse la GPU porbar con arrancar devuleta desde aqui"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggTqH9EPBI5L"
      },
      "outputs": [],
      "source": [
        "!pip install -U bitsandbytes -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade scipy scikit-learn -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xq3sCPMiOCmk",
        "outputId": "5b072590-a167-417e-e7ce-e5ed51dd2eab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.14.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy<2.3,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.26.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEz78AxfBI5M",
        "outputId": "ae0e97b0-8189-4f3e-c5a5-b156e781b045"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "#Reiniciar kernel para que la instalaciones sirvan!\n",
        "import IPython\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_Xo8AvR_KGG"
      },
      "source": [
        "# /apis\n",
        "Adaptación de [apis](https://github.com/AI-secure/aug-pe/tree/main/apis)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ofGqQ3DBXLf"
      },
      "source": [
        "## api.py\n",
        "Definición de la clase API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HesVM89BWXv"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "# Definicion de la clase API después mejor especificada en HFAPI\n",
        "class API(ABC):\n",
        "    def __init__(self, args=None):\n",
        "        self.args = args\n",
        "\n",
        "    @staticmethod\n",
        "    def command_line_parser():\n",
        "        parser = argparse.ArgumentParser()\n",
        "        parser.add_argument(\n",
        "            '--api_help',\n",
        "            action='help')\n",
        "        return parser\n",
        "\n",
        "    @classmethod\n",
        "    def from_command_line_args(cls, args):\n",
        "        \"\"\"\n",
        "        Creating the API from command line arguments.\n",
        "\n",
        "        Args:\n",
        "            args: (List[str]):\n",
        "            The command line arguments\n",
        "        Returns:\n",
        "            API:\n",
        "                The API object.\n",
        "        \"\"\"\n",
        "        args = cls.command_line_parser().parse_args(args)\n",
        "        print(args)\n",
        "        return cls(**vars(args), args=args)\n",
        "\n",
        "    @abstractmethod\n",
        "    def text_random_sampling(self, num_samples, prompt_counter=None):\n",
        "\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def text_variation(self, images, additional_info,\n",
        "                       num_variations_per_image, size, variation_degree=None):\n",
        "\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jtG5qak_Ocn"
      },
      "source": [
        "## hf_api.py\n",
        "Definición de la clase HFAPI para modelos de huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJEl13v3-05G"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "import transformers\n",
        "import random\n",
        "import re\n",
        "import collections\n",
        "\n",
        "\n",
        "# Definicion de HFAPI para utilizar con modelos abiertos de hugging face\n",
        "class HFAPI(API):\n",
        "\n",
        "    def __init__(self,\n",
        "                 model_type, variation_type, use_subcategory,\n",
        "                 output_dir, seed, mlm_probability,\n",
        "                 length, temperature, top_k, top_p, repetition_penalty, do_sample, fp16, no_cuda,\n",
        "                 random_sampling_batch_size, num_beams, dry_run,\n",
        "                 variation_batch_size,\n",
        "                 *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Initializes an object for managing a language model's configuration, tokenizer, and generation process.\n",
        "        Args:\n",
        "            model_type (str): Type or name of the model (e.g., \"gpt2\", \"bert\").\n",
        "            variation_type (str): Type of variation method to apply during text generation.\n",
        "            use_subcategory (bool): Whether to use specific subcategories for certain datasets.\n",
        "            output_dir (str): Directory where outputs or models will be saved.\n",
        "            seed (int): Random seed for reproducibility.\n",
        "            mlm_probability (float): Masked language model probability (if applicable).\n",
        "            length (int): Maximum length for generated text.\n",
        "            temperature (float): Sampling temperature for randomness control in generation.\n",
        "            top_k (int): Top-k sampling parameter for limiting candidate words.\n",
        "            top_p (float): Top-p sampling parameter for nucleus sampling.\n",
        "            repetition_penalty (float): Penalty to avoid repetitive text generation.\n",
        "            do_sample (bool): Whether to use sampling for text generation.\n",
        "            fp16 (bool): Whether to use 16-bit floating-point precision for model inference.\n",
        "            no_cuda (bool): Disable CUDA usage, forcing CPU.\n",
        "            random_sampling_batch_size (int): Batch size for random sampling operations.\n",
        "            num_beams (int): Number of beams for beam search.\n",
        "            dry_run (bool): If True, only simulate the operations without generating results.\n",
        "            variation_batch_size (int): Batch size for generating variations.\n",
        "            *args, **kwargs: Additional arguments for flexibility.\n",
        "        \"\"\"\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        # Assign basic parameters to the instance\n",
        "        self.model_type = model_type\n",
        "        self.variation_type = variation_type\n",
        "        self.output_dir = output_dir\n",
        "        self.length = length\n",
        "        self.temperature = temperature\n",
        "        self.k = top_k\n",
        "        self.p = top_p\n",
        "        self.repetition_penalty = repetition_penalty\n",
        "        self.num_beams = num_beams\n",
        "        self.do_sample = do_sample\n",
        "        self.fp16 = fp16\n",
        "        self.no_cuda = no_cuda\n",
        "        self.seed = seed\n",
        "\n",
        "        # Determine the device: Use GPU if available and not disabled\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() and not self.no_cuda else \"cpu\")\n",
        "        # Determine the number of GPUs to use\n",
        "        self.n_gpu = 0 if self.no_cuda else torch.cuda.device_count()\n",
        "\n",
        "        # Set the random seed for reproducibility\n",
        "        set_seed(seed=seed, n_gpu=self.n_gpu)\n",
        "\n",
        "        # Store whether it's a dry run (test mode)\n",
        "        self.dry_run = dry_run\n",
        "\n",
        "        # Handle subcategory usage if enabled\n",
        "        self.use_subcategory = use_subcategory\n",
        "        if use_subcategory:\n",
        "            # Initialize a dictionary for subcategory mappings for different datasets\n",
        "            self.subcategory_dict = {}\n",
        "            self.subcategory_dict['yelp'] = get_subcategories(\"yelp\")\n",
        "            self.subcategory_dict['pubmed'] = get_subcategories(\"pubmed\")\n",
        "            self.subcategory_dict['openreview'] = get_subcategories(\"openreview\")\n",
        "\n",
        "        # Model name or path for loading the tokenizer and model\n",
        "        model_name_or_path = self.model_type\n",
        "\n",
        "        # Initialize the tokenizer\n",
        "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "            model_name_or_path,\n",
        "            # Optional: Device map can be set for tokenizer\n",
        "            # device_map=\"auto\"\n",
        "        )\n",
        "        # Configure padding token and side\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.tokenizer.padding_side = \"left\"\n",
        "\n",
        "        # Initialize the model\n",
        "        if \"gpt2\" not in self.model_type:\n",
        "            # Load the model in 4-bit precision for large language models\n",
        "            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "                model_name_or_path,\n",
        "                load_in_4bit=True,\n",
        "                device_map=\"auto\",\n",
        "                # Uncomment for 16-bit floating-point precision\n",
        "                # torch_dtype=torch.float16\n",
        "            )\n",
        "        else:\n",
        "            # Special handling for GPT-2 models\n",
        "            pad_token_id = self.tokenizer.pad_token_id if self.tokenizer.pad_token_id else self.tokenizer.eos_token_id\n",
        "            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "                model_name_or_path,\n",
        "                # Uncomment to enable automatic device mapping\n",
        "                # device_map=\"auto\",\n",
        "                pad_token_id=pad_token_id\n",
        "            )\n",
        "            # Use half-precision if specified\n",
        "            if self.fp16:\n",
        "                self.model.half()\n",
        "\n",
        "        # Store batch sizes for different sampling modes\n",
        "        self.random_sampling_batch_size = random_sampling_batch_size\n",
        "        self.variation_batch_size = variation_batch_size\n",
        "\n",
        "    def text_random_sampling(self, num_samples, prompt_counter=None, lens_dict=None):\n",
        "        ratio_generation_training = num_samples / sum(prompt_counter.values())\n",
        "        all_sequences = []\n",
        "        ppls_cur = []\n",
        "        additional_info = []\n",
        "        sync_labels_counter = collections.Counter()\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        simulate_num = 0\n",
        "        for prompt in tqdm(prompt_counter):\n",
        "            # generation is proportional to the label distributions\n",
        "            simulate_num_seq_to_generate = round(prompt_counter[prompt] * ratio_generation_training)\n",
        "            simulate_num += simulate_num_seq_to_generate\n",
        "\n",
        "        print(f\"should -- simulated generated sequences: %d\", simulate_num)\n",
        "        all_prefix_prompts = []\n",
        "        for prompt in tqdm(prompt_counter):\n",
        "            # generation is proportional to the label distributions\n",
        "            num_seq_to_generate = round(prompt_counter[prompt] * ratio_generation_training)\n",
        "            if self.use_subcategory:\n",
        "                full_prompt_text = 'Sos un policia espia. Por sospechas de subversivos fuentes anonimas te informan sobre intinerarios o movimientos de vecinos o conocidos. Escribi una transcripción un informe que te entrego un informante privado.'\n",
        "\n",
        "            else:\n",
        "                full_prompt_text = prompt\n",
        "\n",
        "            prompt_input_ids = self.tokenizer(full_prompt_text)['input_ids']\n",
        "            before_gen_length = len(full_prompt_text)\n",
        "            print('num_seq_to_generate=',num_seq_to_generate)\n",
        "            if num_seq_to_generate > 0:\n",
        "                # condition on the prompt\n",
        "                sequences = self._generate_text(prompt=prompt_input_ids,\n",
        "                                                seq_num=num_seq_to_generate,\n",
        "                                                max_length=self.length,\n",
        "                                                batch_size=self.random_sampling_batch_size,\n",
        "                                                before_gen_length=before_gen_length)\n",
        "                all_sequences += sequences\n",
        "            all_prefix_prompts += [full_prompt_text] * num_seq_to_generate\n",
        "            additional_info += [prompt] * num_seq_to_generate\n",
        "            sync_labels_counter[prompt] = num_seq_to_generate\n",
        "\n",
        "        print(f\"Total generated sequences: %d\", len(all_sequences))\n",
        "        torch.cuda.empty_cache()\n",
        "        return all_sequences,  additional_info, sync_labels_counter, all_prefix_prompts\n",
        "\n",
        "    def _generate_text(self, prompt, seq_num, max_length, batch_size, before_gen_length):\n",
        "\n",
        "        all_data = []\n",
        "\n",
        "        if seq_num < batch_size:\n",
        "            batch_size = seq_num + 1  # TODO: improve\n",
        "\n",
        "        num_return_sequences = 2 if batch_size > 1 else 1\n",
        "        for i in tqdm(range(seq_num // batch_size + 1)):\n",
        "            if self.dry_run:\n",
        "                generated_sequences = [\"s\" * max_length] * batch_size\n",
        "            else:\n",
        "                input_ids = torch.tensor(prompt).repeat(\n",
        "                    batch_size, 1).to(self.device)\n",
        "                with torch.no_grad():\n",
        "                    output_sequences = self.model.generate(\n",
        "                        input_ids=input_ids,\n",
        "                        max_new_tokens=max_length,\n",
        "                        pad_token_id=self.tokenizer.eos_token_id,\n",
        "                        temperature=self.temperature,\n",
        "                        top_k=self.k,\n",
        "                        top_p=self.p,\n",
        "                        num_beams = self.num_beams,\n",
        "                        early_stopping=True,\n",
        "                        repetition_penalty=self.repetition_penalty,\n",
        "                        do_sample=self.do_sample,\n",
        "                        # overgenerate to ensure we have enough non-empty generated sequences\n",
        "                        num_return_sequences=1,\n",
        "                        # num_return_sequences=num_return_sequences,\n",
        "                        # no_repeat_ngram_size=2,\n",
        "                    )\n",
        "                    generated_sequences = self.tokenizer.batch_decode(output_sequences[:, input_ids.shape[1]:],\n",
        "                                                                      skip_special_tokens=True,\n",
        "                                                                      clean_up_tokenization_spaces=True)\n",
        "            for g in generated_sequences:\n",
        "                seq = g\n",
        "                seq = \" \".join(seq.split())\n",
        "                if seq:\n",
        "                    all_data.append(seq)\n",
        "\n",
        "        if len(all_data) > seq_num:\n",
        "            all_data = random.sample(all_data, seq_num)\n",
        "        return all_data\n",
        "\n",
        "    def text_variation(self, sequences, additional_info,\n",
        "                       num_variations_per_sequence, variation_degree):\n",
        "        self.model.eval()\n",
        "        # self.model.to(self.device)\n",
        "        variations = []\n",
        "        for idx in tqdm(range(num_variations_per_sequence)):\n",
        "            sub_variations, var_labels = self._text_variation(\n",
        "                sequences=sequences,\n",
        "                labels=list(additional_info),\n",
        "                variation_degree=variation_degree,\n",
        "                variation_type=self.variation_type,\n",
        "                batch_size=self.variation_batch_size)\n",
        "            variations.append(sub_variations)\n",
        "        torch.cuda.empty_cache()\n",
        "        return np.stack(variations, axis=1), var_labels, [], [], []\n",
        "\n",
        "    def _rephrase(self, label, sequence, variation_type):\n",
        "\n",
        "        selected_style = ALL_PUBMED_styles[random.randrange(len(ALL_PUBMED_styles))]\n",
        "        prompt = \"Por favor reformula las siguientes oraciones {} como la transcripción de un informe policial:\\n{} \\n\".format(\n",
        "           selected_style, sequence)\n",
        "        # prompt = \"Por favor reformula las siguientes oraciones {} para hablar sobre el aspecto de una persona:\\n{} \\n\".format(\n",
        "        #     selected_style, sequence)\n",
        "        return prompt\n",
        "\n",
        "    def _text_variation(self, sequences, labels, variation_degree, variation_type, batch_size):\n",
        "        if self.dry_run:\n",
        "            all_data = [seq+\"s\"*self.length for seq in sequences]\n",
        "            all_labels = [lab for lab in labels]\n",
        "            return all_data, all_labels\n",
        "\n",
        "        num_seq = len(sequences)\n",
        "        all_data = []\n",
        "        all_labels = []\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        self.mlm_probability = variation_degree\n",
        "\n",
        "        for i in tqdm(range(num_seq // batch_size + 1)):\n",
        "            start_idx = i*batch_size\n",
        "            if start_idx >= num_seq:\n",
        "                break\n",
        "            end_idx = num_seq if (\n",
        "                i+1)*batch_size > num_seq else (i+1)*batch_size\n",
        "\n",
        "            batch_prompt = []\n",
        "            batch_labels = []\n",
        "            for idx in range(start_idx, end_idx):\n",
        "                prompt = self._rephrase(\n",
        "                    labels[idx], sequences[idx], variation_type)\n",
        "                batch_prompt.append(prompt)\n",
        "                batch_labels.append(labels[idx])\n",
        "\n",
        "            with torch.no_grad():\n",
        "                input_ids = self.tokenizer(batch_prompt, padding=True, return_tensors='pt')[\n",
        "                    'input_ids'].to(self.device)  # has been padded into the same lens; cannot be used\n",
        "                beam_output = self.model.generate(\n",
        "                        input_ids=input_ids,\n",
        "                        max_new_tokens=self.length,\n",
        "                        pad_token_id=self.tokenizer.eos_token_id,\n",
        "                        temperature=self.temperature,\n",
        "                        top_k=self.k,\n",
        "                        top_p=self.p,\n",
        "                        early_stopping=True,\n",
        "                        repetition_penalty=self.repetition_penalty,\n",
        "                        do_sample=self.do_sample,\n",
        "                        # overgenerate to ensure we have enough non-empty generated sequences\n",
        "                        num_return_sequences=1,\n",
        "                        # num_return_sequences=num_return_sequences,\n",
        "                        # no_repeat_ngram_size=2,\n",
        "                    )\n",
        "                # TODO:   skip the tokens so the lens of input_ids is diff from batch_prompt\n",
        "                generated_sequences = self.tokenizer.batch_decode(\n",
        "                    beam_output[:, input_ids.shape[1]:], skip_special_tokens=True,  clean_up_tokenization_spaces=True)\n",
        "            for idx in range(len(generated_sequences)):\n",
        "                seq = generated_sequences[idx]\n",
        "                seq = \" \".join(seq.split())\n",
        "                lab = batch_labels[idx].strip().split(\"\\t\")\n",
        "                if seq:\n",
        "                    all_data.append(seq)  # no lables!\n",
        "                else:\n",
        "                    all_data.append(batch_prompt[idx])\n",
        "                all_labels.append(lab)\n",
        "\n",
        "        # logging.info(f\" _text_variation output lens  {len(all_data)}\")\n",
        "\n",
        "        return all_data, all_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iy7rIDLbB1s6"
      },
      "source": [
        "## utils.py (prompts)\n",
        "Definición de prompts de variación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfmc5UWNB83i"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import functools\n",
        "import signal\n",
        "\n",
        "\n",
        "# Variation Prompts\n",
        "ALL_PUBMED_styles = [\"de forma casual\", \"de forma creativa\",  \"de forma concisa\", \"de forma cronológica\"]\n",
        "\n",
        "\n",
        "def set_seed(seed, n_gpu=0):\n",
        "    import random  # Import the random module inside the function\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    if n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "class Timer:\n",
        "    \"\"\"Timer context manager\"\"\"\n",
        "\n",
        "    def __enter__(self):\n",
        "        \"\"\"Start a new timer as a context manager\"\"\"\n",
        "        self.start = time.time()\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, *args):\n",
        "        \"\"\"Stop the context manager timer\"\"\"\n",
        "        self.end = time.time()\n",
        "        self.duration = self.end - self.start\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"{self.duration:.1f} seconds\"\n",
        "\n",
        "\n",
        "def timeout(sec):\n",
        "    \"\"\"\n",
        "    timeout decorator\n",
        "    :param sec: function raise TimeoutError after ? seconds\n",
        "    \"\"\"\n",
        "    def decorator(func):\n",
        "        @functools.wraps(func)\n",
        "        def wrapped_func(*args, **kwargs):\n",
        "\n",
        "            def _handle_timeout(signum, frame):\n",
        "                err_msg = f'Function {func.__name__} timed out after {sec} seconds'\n",
        "                raise TimeoutError(err_msg)\n",
        "\n",
        "            signal.signal(signal.SIGALRM, _handle_timeout)\n",
        "            signal.alarm(sec)\n",
        "            try:\n",
        "                result = func(*args, **kwargs)\n",
        "            finally:\n",
        "                signal.alarm(0)\n",
        "            return result\n",
        "\n",
        "        return wrapped_func\n",
        "    return decorator\n",
        "\n",
        "# Nosotros no lo usamos\n",
        "def get_subcategories(dataset):\n",
        "    if \"yelp\" in dataset:\n",
        "        category_list = {'Restaurants', 'Bars', 'Shopping', 'Event Planning & Services',\n",
        "                         'Beauty & Spas', 'Arts & Entertainment', 'Hotels & Travel',\n",
        "                         'Health & Medical', 'Grocery', 'Home & Garden'}\n",
        "\n",
        "        subcategory_list = {}\n",
        "        for cate in category_list:\n",
        "            prefix = cate.lower().split(' ')[0]\n",
        "            fname = f'data/yelp/subcategories/{prefix}.txt'\n",
        "            file1 = open(fname, 'r')\n",
        "            Lines = file1.readlines()\n",
        "            Lines = [s.replace('\\n', '') for s in Lines]\n",
        "            subcategory_list[cate] = Lines\n",
        "        # print(subcategory_list)\n",
        "    elif \"pubmed\" in dataset:\n",
        "        fname = f'data/pubmed/writers.txt'\n",
        "        file1 = open(fname, 'r')\n",
        "        Lines = file1.readlines()\n",
        "        Lines = [s.replace('\\n', '') for s in Lines]\n",
        "        subcategory_list = Lines\n",
        "    elif \"openreview\" in dataset:\n",
        "        fname = f'data/openreview/writers.txt'\n",
        "        file1 = open(fname, 'r')\n",
        "        Lines = file1.readlines()\n",
        "        Lines = [s.replace('\\n', '').replace(':', \" who has\") for s in Lines]\n",
        "        subcategory_list = Lines\n",
        "\n",
        "    return subcategory_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqAaHBNYGJPs"
      },
      "source": [
        "# /dpsda\n",
        "Adaptación de [dpsda](https://github.com/AI-secure/aug-pe/tree/main/dpsda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4_ppDm_Goj_"
      },
      "source": [
        "## feature_extractor.py\n",
        "Para calcular los embeddinggs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkSEcfZpErlG"
      },
      "outputs": [],
      "source": [
        "!pip install sentence_transformers -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "def extract_features(\n",
        "        data,\n",
        "        batch_size=1000,\n",
        "        model_name=\"all-mpnet-base-v2\"):\n",
        "    \"\"\"\n",
        "    Extracts sentence embeddings from a given dataset using a specified model.\n",
        "\n",
        "    Args:\n",
        "        data (list or array-like): A list or array of sentences to process.\n",
        "        batch_size (int): Number of sentences to process in each batch. Default is 1000.\n",
        "        model_name (str): The name of the Sentence Transformer model to use. Default is \"all-mpnet-base-v2\".\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A numpy array containing the concatenated embeddings for each sentence.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the SentenceTransformer model with the specified model name.\n",
        "    # Optionally, a 'device' parameter could be set here to specify GPU usage (e.g., device='cuda').\n",
        "    model = SentenceTransformer(model_name)\n",
        "    model.eval()  # Set the model to evaluation mode to avoid training behavior.\n",
        "\n",
        "    # Disable gradient calculations to save memory and improve computation speed (since we're just encoding).\n",
        "    with torch.no_grad():\n",
        "        sentence_embeddings = []  # Initialize an empty list to hold embeddings for each batch.\n",
        "\n",
        "        # Loop over data in batches, with tqdm providing a progress bar.\n",
        "        for i in tqdm(range(len(data) // batch_size + 1)):\n",
        "            # Get embeddings for the current batch of data.\n",
        "            # Slices the 'data' array to get the current batch.\n",
        "            embeddings = model.encode(\n",
        "                data[i * batch_size:(i + 1) * batch_size])\n",
        "\n",
        "            # Only append if the embeddings array has content.\n",
        "            if len(embeddings) > 0:\n",
        "                sentence_embeddings.append(embeddings)\n",
        "\n",
        "    # Concatenate all the batches of embeddings into a single numpy array.\n",
        "    sentence_embeddings = np.concatenate(sentence_embeddings)\n",
        "\n",
        "    # Delete the model from memory to free up resources.\n",
        "    del model\n",
        "\n",
        "    # Return the final array of sentence embeddings.\n",
        "    return sentence_embeddings"
      ],
      "metadata": {
        "id": "vAZiR6-VoHhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q56xwY4rGjX-"
      },
      "source": [
        "## dp_counter.py\n",
        "Encargado del histograma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhMVPdr4E_2Z"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-gpu -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIszcTkMGl33"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "import logging\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import torch\n",
        "\n",
        "\n",
        "def dp_nn_histogram(public_features, private_features, noise_multiplier,\n",
        "                    num_packing=1, num_nearest_neighbor=1, mode='L2',\n",
        "                    threshold=0.0):\n",
        "    assert public_features.shape[0] % num_packing == 0\n",
        "\n",
        "    num_true_public_features = public_features.shape[0] // num_packing\n",
        "    if public_features.shape[0] == 0:  # TODO debug, why this case exists\n",
        "        return np.zeros(shape=num_true_public_features), np.zeros(shape=num_true_public_features)\n",
        "\n",
        "    faiss_res = faiss.StandardGpuResources()\n",
        "    if mode == 'L2':\n",
        "        index = faiss.IndexFlatL2(public_features.shape[1])\n",
        "    # inner product; need normalization (https://github.com/spotify/annoy)\n",
        "    elif mode == 'IP':\n",
        "        index = faiss.IndexFlatIP(public_features.shape[1])\n",
        "    elif mode == 'cos_sim':\n",
        "        # normalize the embeddings first\n",
        "        faiss.normalize_L2(public_features)\n",
        "        faiss.normalize_L2(private_features)\n",
        "        index = faiss.IndexFlatIP(public_features.shape[1])\n",
        "    else:\n",
        "        raise Exception(f'Unknown mode {mode}')\n",
        "    if torch.cuda.is_available():\n",
        "        index = faiss.index_cpu_to_gpu(faiss_res, 0, index)\n",
        "\n",
        "    print(f'public_features shape : {public_features.shape}')\n",
        "    print(f'private_features shape : {private_features.shape}')\n",
        "\n",
        "    index.add(public_features)\n",
        "    print(f'Number of samples in index: {index.ntotal}')\n",
        "    distance, ids = index.search(private_features, k=num_nearest_neighbor)\n",
        "    print('Finished search')\n",
        "\n",
        "    counter = Counter(list(ids.flatten()))\n",
        "    # shape of the synthetic samples\n",
        "    count = np.zeros(shape=num_true_public_features)\n",
        "    for k in counter:\n",
        "        count[k % num_true_public_features] += counter[k]\n",
        "    print(f'Clean count: {count}')\n",
        "    print(f'Clean count sum: {np.sum(count)}')\n",
        "    print(f'Clean count num>0: {np.sum(count > 0)}')\n",
        "    print(f'Largest clean counters: {sorted(count)[::-1][:50]}')\n",
        "    count = np.asarray(count)\n",
        "    clean_count = count.copy()\n",
        "    count += (np.random.normal(size=len(count)) * np.sqrt(num_nearest_neighbor)\n",
        "              * noise_multiplier)\n",
        "    print(f'Noisy count sum: {np.sum(count)}')\n",
        "    print(f'Noisy count num>0: {np.sum(count > 0)}')\n",
        "    print(f'Largest noisy counters: {sorted(count)[::-1][:50]}')\n",
        "    count = np.clip(count, a_min=threshold, a_max=None)\n",
        "    count = count - threshold\n",
        "    print(f'Clipped noisy count sum: {np.sum(count)}')\n",
        "    print(f'Clipped noisy count num>0: {np.sum(count > 0)}')\n",
        "    print(f'Clipped largest noisy counters: {sorted(count)[::-1][:50]}')\n",
        "    torch.cuda.empty_cache()\n",
        "    return count, clean_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZJyuf3z2uDx"
      },
      "source": [
        "## data_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Tvn_UNLFrzT"
      },
      "outputs": [],
      "source": [
        "!pip install datasets -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-Zp1n0o2ohB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import logging\n",
        "import collections\n",
        "import csv\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "def sample_dataset(data_name, dataset, label_column_name='label1', sample_size=5000, subsample_one_class=False):\n",
        "    # Muestra una parte del dataset según el tamaño especificado\n",
        "    print(f\"sample_size: {sample_size}\")\n",
        "    if subsample_one_class == False and sample_size < 0:\n",
        "        return dataset  # Retorna dataset completo si no hay muestreo\n",
        "\n",
        "    training_dataset = dataset\n",
        "    sample_indices = []\n",
        "\n",
        "    if subsample_one_class:\n",
        "        if sample_size < 0:\n",
        "            sample_indices = indices\n",
        "        else:\n",
        "            # Muestra aleatoria de una clase si subsample_one_class es True\n",
        "            sample_indices = np.random.choice(indices, size=sample_size, replace=False)\n",
        "            np.random.shuffle(sample_indices)\n",
        "    else:\n",
        "        # Muestra aleatoria general del dataset\n",
        "        indices = list(range(len(training_dataset)))\n",
        "        sample_indices = np.random.choice(indices, size=sample_size, replace=False)\n",
        "        np.random.shuffle(sample_indices)\n",
        "\n",
        "    print(sample_indices)\n",
        "\n",
        "    # Filtra el dataset usando los índices seleccionados\n",
        "    training_dataset = training_dataset.select(sample_indices)\n",
        "    dataset = training_dataset\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def load_data(dataset, data_file, num_samples=-1, subsample_one_class=False, gen=False):\n",
        "    # Carga y preprocesa datos, asignando prompts como etiquetas\n",
        "    print(\"data_file\", data_file)\n",
        "    prompt_counter = collections.Counter()\n",
        "    raw_datasets = data_file\n",
        "\n",
        "    # Muestra datos del dataset original\n",
        "    original_data = sample_dataset(dataset, raw_datasets, label_column_name='',\n",
        "                                    sample_size=num_samples, subsample_one_class=subsample_one_class)\n",
        "    prompt_idexer = dict()  # Almacena índices por cada prompt\n",
        "    train_data = []\n",
        "    train_labels = []\n",
        "    for i, line in enumerate(original_data):\n",
        "\n",
        "        #los prompt son labels\n",
        "        prompt = f\"dinosaurio\"  # Etiqueta fija asignada a cada dato\n",
        "        prompt_counter[prompt] += 1 # Cuenta ocurrencias del prompt\n",
        "\n",
        "        # Asocia índices al prompt actual\n",
        "        if prompt not in prompt_idexer.keys():\n",
        "            prompt_idexer[prompt] = [i]\n",
        "        else:\n",
        "            prompt_idexer[prompt].append(i)\n",
        "        train_data.append(line['text']) # Guarda texto de entrada\n",
        "        train_labels.append(prompt) # Guarda etiqueta asignada\n",
        "    return train_data, train_labels, prompt_counter, prompt_idexer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn9L3IFLGuWg"
      },
      "source": [
        "## metrics.py\n",
        "Algunas métricas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08J6WtgxG-By"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from time import time\n",
        "from numpy import cov\n",
        "from numpy import trace\n",
        "from numpy import iscomplexobj\n",
        "from numpy.random import random\n",
        "from scipy.linalg import sqrtm\n",
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "\n",
        "\n",
        "# Frechet Inception Distance\n",
        "def calculate_fid(act1, act2):\n",
        "    # calculate mean and covariance statistics\n",
        "    mu1, sigma1 = act1.mean(axis=0), cov(act1, rowvar=False)\n",
        "    mu2, sigma2 = act2.mean(axis=0), cov(act2, rowvar=False)\n",
        "    # calculate sum squared difference between means\n",
        "    ssdiff = np.sum((mu1 - mu2) ** 2.0)\n",
        "    # calculate sqrt of product between cov\n",
        "    covmean = sqrtm(sigma1.dot(sigma2))\n",
        "    # check and correct imaginary numbers from sqrt\n",
        "    if iscomplexobj(covmean):\n",
        "        covmean = covmean.real\n",
        "    # calculate score\n",
        "    fid = ssdiff + trace(sigma1 + sigma2 - 2.0 * covmean)\n",
        "    return fid\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# https://github.com/kynkaat/improved-precision-and-recall-metric/blob/master/precision_recall.py\n",
        "class DistanceBlock():\n",
        "    \"\"\"Provides multi-GPU support to calculate pairwise distances between two batches of feature vectors.\"\"\"\n",
        "\n",
        "    def __init__(self, num_features, num_gpus):\n",
        "        self.num_features = num_features\n",
        "        self.num_gpus = num_gpus\n",
        "\n",
        "    def pairwise_distances(self, U, V):\n",
        "        \"\"\"Evaluate pairwise distances between two batches of feature vectors.\"\"\"\n",
        "        output = pairwise_distances(U, V, n_jobs=24)\n",
        "        return output\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "class ManifoldEstimator():\n",
        "    \"\"\"Estimates the manifold of given feature vectors.\"\"\"\n",
        "\n",
        "    def __init__(self, distance_block, features, row_batch_size=25000, col_batch_size=50000,\n",
        "                 nhood_sizes=[3], clamp_to_percentile=None, eps=1e-5):\n",
        "        \"\"\"Estimate the manifold of given feature vectors.\n",
        "\n",
        "            Args:\n",
        "                distance_block: DistanceBlock object that distributes pairwise distance\n",
        "                    calculation to multiple GPUs.\n",
        "                features (np.array/tf.Tensor): Matrix of feature vectors to estimate their manifold.\n",
        "                row_batch_size (int): Row batch size to compute pairwise distances\n",
        "                    (parameter to trade-off between memory usage and performance).\n",
        "                col_batch_size (int): Column batch size to compute pairwise distances.\n",
        "                nhood_sizes (list): Number of neighbors used to estimate the manifold.\n",
        "                clamp_to_percentile (float): Prune hyperspheres that have radius larger than\n",
        "                    the given percentile.\n",
        "                eps (float): Small number for numerical stability.\n",
        "        \"\"\"\n",
        "        num_images = features.shape[0]\n",
        "        self.nhood_sizes = nhood_sizes\n",
        "        self.num_nhoods = len(nhood_sizes)\n",
        "        self.eps = eps\n",
        "        self.row_batch_size = row_batch_size\n",
        "        self.col_batch_size = col_batch_size\n",
        "        self._ref_features = features\n",
        "        self._distance_block = distance_block\n",
        "\n",
        "        # Estimate manifold of features by calculating distances to k-NN of each sample.\n",
        "        self.D = np.zeros([num_images, self.num_nhoods], dtype=np.float32)\n",
        "        distance_batch = np.zeros(\n",
        "            [row_batch_size, num_images], dtype=np.float32)\n",
        "        seq = np.arange(max(self.nhood_sizes) + 1, dtype=np.int32)\n",
        "\n",
        "        for begin1 in range(0, num_images, row_batch_size):\n",
        "            end1 = min(begin1 + row_batch_size, num_images)\n",
        "            row_batch = features[begin1:end1]\n",
        "\n",
        "            for begin2 in range(0, num_images, col_batch_size):\n",
        "                end2 = min(begin2 + col_batch_size, num_images)\n",
        "                col_batch = features[begin2:end2]\n",
        "\n",
        "                # Compute distances between batches.\n",
        "                distance_batch[0:end1 - begin1, begin2:end2] = self._distance_block.pairwise_distances(row_batch,\n",
        "                                                                                                       col_batch)\n",
        "\n",
        "            # Find the k-nearest neighbor from the current batch.\n",
        "            self.D[begin1:end1, :] = np.partition(\n",
        "                distance_batch[0:end1 - begin1, :], seq, axis=1)[:, self.nhood_sizes]\n",
        "\n",
        "        if clamp_to_percentile is not None:\n",
        "            max_distances = np.percentile(self.D, clamp_to_percentile, axis=0)\n",
        "            self.D[self.D > max_distances] = 0\n",
        "\n",
        "    def evaluate(self, eval_features, return_realism=False, return_neighbors=False):\n",
        "        \"\"\"Evaluate if new feature vectors are at the manifold.\"\"\"\n",
        "        num_eval_images = eval_features.shape[0]\n",
        "        num_ref_images = self.D.shape[0]\n",
        "        distance_batch = np.zeros(\n",
        "            [self.row_batch_size, num_ref_images], dtype=np.float32)\n",
        "        batch_predictions = np.zeros(\n",
        "            [num_eval_images, self.num_nhoods], dtype=np.int32)\n",
        "        max_realism_score = np.zeros([num_eval_images, ], dtype=np.float32)\n",
        "        nearest_indices = np.zeros([num_eval_images, ], dtype=np.int32)\n",
        "\n",
        "        for begin1 in range(0, num_eval_images, self.row_batch_size):\n",
        "            end1 = min(begin1 + self.row_batch_size, num_eval_images)\n",
        "            feature_batch = eval_features[begin1:end1]\n",
        "\n",
        "            for begin2 in range(0, num_ref_images, self.col_batch_size):\n",
        "                end2 = min(begin2 + self.col_batch_size, num_ref_images)\n",
        "                ref_batch = self._ref_features[begin2:end2]\n",
        "\n",
        "                distance_batch[0:end1 - begin1, begin2:end2] = self._distance_block.pairwise_distances(feature_batch,\n",
        "                                                                                                       ref_batch)\n",
        "\n",
        "            # From the minibatch of new feature vectors, determine if they are in the estimated manifold.\n",
        "            # If a feature vector is inside a hypersphere of some reference sample, then\n",
        "            # the new sample lies at the estimated manifold.\n",
        "            # The radii of the hyperspheres are determined from distances of neighborhood size k.\n",
        "            samples_in_manifold = distance_batch[0:end1 -\n",
        "                                                 begin1, :, None] <= self.D\n",
        "            batch_predictions[begin1:end1] = np.any(\n",
        "                samples_in_manifold, axis=1).astype(np.int32)\n",
        "\n",
        "            max_realism_score[begin1:end1] = np.max(self.D[:, 0] / (distance_batch[0:end1 - begin1, :] + self.eps),\n",
        "                                                    axis=1)\n",
        "            nearest_indices[begin1:end1] = np.argmin(\n",
        "                distance_batch[0:end1 - begin1, :], axis=1)\n",
        "\n",
        "        if return_realism and return_neighbors:\n",
        "            return batch_predictions, max_realism_score, nearest_indices\n",
        "        elif return_realism:\n",
        "            return batch_predictions, max_realism_score\n",
        "        elif return_neighbors:\n",
        "            return batch_predictions, nearest_indices\n",
        "\n",
        "        return batch_predictions\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "def knn_precision_recall_features(ref_features, eval_features, nhood_sizes=[3],\n",
        "                                  row_batch_size=10000, col_batch_size=50000, num_gpus=1, debug=True):\n",
        "    \"\"\"Calculates k-NN precision and recall for two sets of feature vectors.\n",
        "\n",
        "        Args:\n",
        "            ref_features (np.array/tf.Tensor): Feature vectors of reference images.\n",
        "            eval_features (np.array/tf.Tensor): Feature vectors of generated images.\n",
        "            nhood_sizes (list): Number of neighbors used to estimate the manifold.\n",
        "            row_batch_size (int): Row batch size to compute pairwise distances\n",
        "                (parameter to trade-off between memory usage and performance).\n",
        "            col_batch_size (int): Column batch size to compute pairwise distances.\n",
        "            num_gpus (int): Number of GPUs used to evaluate precision and recall.\n",
        "\n",
        "        Returns:\n",
        "            State (dict): Dict that contains precision and recall calculated from\n",
        "            ref_features and eval_features.\n",
        "    \"\"\"\n",
        "    state = dict()\n",
        "    if debug:\n",
        "        state['precision'] = 0\n",
        "        state['recall'] = 0\n",
        "        state['f1'] = 0\n",
        "        return state\n",
        "\n",
        "    num_images = ref_features.shape[0]\n",
        "    num_features = ref_features.shape[1]\n",
        "\n",
        "    # Initialize DistanceBlock and ManifoldEstimators.\n",
        "    distance_block = DistanceBlock(num_features, num_gpus)\n",
        "    ref_manifold = ManifoldEstimator(\n",
        "        distance_block, ref_features, row_batch_size, col_batch_size, nhood_sizes)\n",
        "    eval_manifold = ManifoldEstimator(\n",
        "        distance_block, eval_features, row_batch_size, col_batch_size, nhood_sizes)\n",
        "\n",
        "    # Evaluate precision and recall using k-nearest neighbors.\n",
        "    print('Evaluating k-NN precision and recall with %i samples...' % num_images)\n",
        "    start = time()\n",
        "\n",
        "    # Precision: How many points from eval_features are in ref_features manifold.\n",
        "    precision = ref_manifold.evaluate(eval_features)\n",
        "    state['precision'] = precision.mean(axis=0).item()\n",
        "\n",
        "    # Recall: How many points from ref_features are in eval_features manifold.\n",
        "    recall = eval_manifold.evaluate(ref_features)\n",
        "    state['recall'] = recall.mean(axis=0).item()\n",
        "\n",
        "    state['f1'] = 2 * (state['precision'] * state['recall']) / \\\n",
        "        (state['precision']+state['recall'])\n",
        "\n",
        "    print('Evaluated k-NN precision and recall in: %gs' % (time() - start))\n",
        "\n",
        "    return state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvBRu-1PHEJS"
      },
      "source": [
        "## logging.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojnLGRZUHHQD"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import numpy as np\n",
        "import csv\n",
        "import json\n",
        "# from dpsda.metrics import calculate_fid, knn_precision_recall_features\n",
        "\n",
        "def compute_fid(synthetic_features, all_private_features, feature_extractor, folder='', step=0, log_online=False):\n",
        "    # Calcula el FID y F1 entre características generadas y reales\n",
        "\n",
        "    print(f'Computing FID and F1 for syn shape {synthetic_features.shape}')\n",
        "    fid = calculate_fid(synthetic_features, all_private_features)\n",
        "    state = knn_precision_recall_features(ref_features=all_private_features,\n",
        "                                          eval_features=synthetic_features)\n",
        "    print(f'fid={fid} F1={state}')\n",
        "\n",
        "\n",
        "\n",
        "def setup_logging(log_file):\n",
        "    # Configura el sistema de logging para consola y archivo\n",
        "\n",
        "    log_formatter = logging.Formatter(\n",
        "        fmt=('%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  '\n",
        "             '%(message)s'),\n",
        "        datefmt='%m/%d/%Y %H:%M:%S %p')\n",
        "    root_logger = logging.getLogger()\n",
        "    # root_logger.setLevel(logging.DEBUG)\n",
        "    root_logger.setLevel(logging.INFO)\n",
        "\n",
        "    console_handler = logging.StreamHandler() # Logging en consola\n",
        "    console_handler.setFormatter(log_formatter)\n",
        "    root_logger.addHandler(console_handler)\n",
        "\n",
        "    file_handler = logging.FileHandler(log_file)  # Logging en archivo\n",
        "    file_handler.setFormatter(log_formatter)\n",
        "    root_logger.addHandler(file_handler)\n",
        "\n",
        "    pil_logger = logging.getLogger('PIL') # Reduce logs innecesarios de PIL\n",
        "    pil_logger.setLevel(logging.INFO)\n",
        "\n",
        "\n",
        "def log_embeddings(embeddings, additional_info, folder, fname=''):\n",
        "    # Guarda embeddings y metadatos en un archivo comprimido\n",
        "\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "    savefname = os.path.join(folder, fname+'.embeddings.npz')\n",
        "    print(\"save embeddings into\", savefname)\n",
        "    np.savez(\n",
        "        savefname,\n",
        "        embeddings=embeddings,\n",
        "        additional_info=additional_info)\n",
        "\n",
        "\n",
        "def load_embeddings(path):\n",
        "    # Carga embeddings y metadatos de un archivo comprimido\n",
        "\n",
        "    data = np.load(path)\n",
        "    embeddings = data['embeddings']\n",
        "    additional_info = data['additional_info']\n",
        "\n",
        "    return embeddings, additional_info\n",
        "\n",
        "\n",
        "def log_num_words(fname=\"num_word_lookahead.csv\", all_gen_words=[], all_target_words=[]):\n",
        "    # Calcula y guarda diferencias estadísticas entre palabras generadas y objetivo\n",
        "\n",
        "    if len(all_gen_words) == 0 or len(all_target_words) == 0:\n",
        "        return\n",
        "    with open(fname, 'w', newline='', encoding=\"utf-8\") as wf:\n",
        "        csv_writer = csv.writer(wf)\n",
        "        csv_writer.writerow([\"target\", \"gen\", \"diff\"])\n",
        "        diff_list = []\n",
        "        diff_abs_list = []\n",
        "        for i in range(len(all_target_words)):\n",
        "            try:\n",
        "                diff_list.append(all_gen_words[i] - all_target_words[i])\n",
        "                diff_abs_list.append(\n",
        "                    abs(all_gen_words[i] - all_target_words[i]))\n",
        "                csv_writer.writerow(\n",
        "                    [all_target_words[i], all_gen_words[i], all_gen_words[i] - all_target_words[i]])\n",
        "            except:\n",
        "                continue\n",
        "        csv_writer.writerow([\"mean_abs\", \"var_abs\", \"mean\", \"var\"])\n",
        "        csv_writer.writerow([np.mean(diff_abs_list), np.std(\n",
        "            diff_abs_list), np.mean(diff_list), np.std(diff_list)])\n",
        "\n",
        "\n",
        "def log_prompt_generation(fname=\"prompt_generation.jsonl\", prompts=[], generations=[]):\n",
        "    # Guarda prompts y sus generaciones asociadas en un archivo JSONL\n",
        "\n",
        "    new_variants_samples = []\n",
        "    for x in generations:\n",
        "        new_variants_samples.extend(x.tolist())\n",
        "\n",
        "    if len(prompts) == 0 or len(new_variants_samples) == 0:\n",
        "        return\n",
        "    with open(fname, \"w\") as file:\n",
        "        for i in range(len(prompts)):\n",
        "            try:\n",
        "                json_str = json.dumps(\n",
        "                    {\"prompt\": prompts[i], \"generation\": new_variants_samples[i]})\n",
        "                file.write(json_str + \"\\n\")\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "\n",
        "def log_count(count, clean_count, path):\n",
        "    # Guarda conteos en un archivo CSV\n",
        "\n",
        "    dirname = os.path.dirname(path)\n",
        "    if not os.path.exists(dirname):\n",
        "        os.makedirs(dirname)\n",
        "\n",
        "    title = ['type', 'count']\n",
        "    with open(path, 'w', newline='', encoding=\"utf-8\") as wf:\n",
        "        csv_writer = csv.writer(wf)\n",
        "        csv_writer.writerow(title)\n",
        "        csv_writer.writerow([\"count\", count.tolist()])\n",
        "        csv_writer.writerow([\"clean_count\", clean_count.tolist()])\n",
        "\n",
        "\n",
        "def log_fid(folder, fid, f1, precision, recall, t, save_fname='fid.csv'):\n",
        "    # Registra métricas de FID, F1, precisión y recall en un archivo CSV\n",
        "\n",
        "    with open(os.path.join(folder, save_fname), 'a') as f:\n",
        "        f.write(f'{t} {fid} {f1} {precision} {recall}\\n')\n",
        "\n",
        "\n",
        "def log_fid_list(folder, fids, t, save_fname='fid.csv'):\n",
        "    # Guarda una lista de FIDs asociados a un paso específico\n",
        "\n",
        "    write_list = [t]\n",
        "    write_list.extend(fids)\n",
        "    with open(os.path.join(folder, save_fname), 'a') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(write_list)\n",
        "\n",
        "\n",
        "def log_samples(samples, additional_info, folder):\n",
        "    # Guarda muestras generadas y sus etiquetas en un archivo CSV\n",
        "\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "\n",
        "    all_data = []\n",
        "    for i in range(len(samples)):\n",
        "        seq = samples[i]\n",
        "        labels = additional_info[i]\n",
        "        if seq:\n",
        "            seq = \" \".join(seq.split()) # Limpia espacios extra\n",
        "            if \"pubmed\" in labels:\n",
        "                all_data.append([seq])\n",
        "            else:\n",
        "                labels = labels.strip().split(\"\\t\")\n",
        "                all_data.append([seq]+labels)\n",
        "\n",
        "    if \"pubmed\" in additional_info[0]:  # unconditional\n",
        "        title = ['text']\n",
        "    else:\n",
        "        title = ['text', 'label1', 'label2']\n",
        "    try:\n",
        "        with open(os.path.join(folder, 'samples.csv'), 'w', newline='', encoding=\"utf-8\") as wf:\n",
        "            csv_writer = csv.writer(wf)\n",
        "            csv_writer.writerow(title)\n",
        "            for obj in all_data:\n",
        "                if obj[0]:  # remove empty sequences\n",
        "                    csv_writer.writerow(obj)\n",
        "    except:  # in case there are some special characters in the text\n",
        "        with open(os.path.join(folder, 'samples.csv'), 'w', newline='', encoding=\"utf-8\") as wf:\n",
        "            csv_writer = csv.writer(\n",
        "                wf, quoting=csv.QUOTE_NONE,  quotechar='', escapechar='\\\\')\n",
        "            csv_writer.writerow(title)\n",
        "            for obj in all_data:\n",
        "                if obj[0]:  # remove empty sequences\n",
        "                    csv_writer.writerow(obj)\n",
        "    return all_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrSE1duJ1Rng"
      },
      "source": [
        "# Data Path UNC\n",
        "Ruta de la carpeta donde estan los archivos (en nuestro drive).\n",
        "Si se quieren descargar: [DRIVE](https://drive.google.com/drive/folders/1YumfaRFwbPIDfc6giQ_13_VKLRb9jtuZ?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q26-jDJ41RKW"
      },
      "outputs": [],
      "source": [
        "folder_path = '/content/drive/Shareddrives/Proyectous/Archivo del Tword/data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzLRE-l43MM6"
      },
      "outputs": [],
      "source": [
        "folder_path = '/content/drive/MyDrive/TextMining/Data'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHxxI3_xreBK"
      },
      "source": [
        "# Dataset\n",
        "Importamos los documentos y armamos el dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rijdgRTrgCY"
      },
      "outputs": [],
      "source": [
        "!pip install datasets -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JM2T_gBermkv",
        "outputId": "81406581-a696-488d-e34c-d1d98712cae5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos los textos con load_txt_files segun donde esten alojados los archivos"
      ],
      "metadata": {
        "id": "pDC0J1xbDGeC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amQjdM2OroHx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "def load_txt_files(folder_path):\n",
        "  \"\"\"\n",
        "  Load contents of all .txt files in a given folder.\n",
        "\n",
        "  Args:\n",
        "    folder_path (str): Path to the folder containing .txt files.\n",
        "\n",
        "  Returns:\n",
        "    list: List of strings, each representing the content of a .txt file.\n",
        "  \"\"\"\n",
        "\n",
        "  # Initialize an empty list to store the contents of .txt files\n",
        "  data = []\n",
        "\n",
        "  # Iterate over each file in the specified folder\n",
        "  for filename in os.listdir(folder_path):\n",
        "    # Check if the file has a .txt extension\n",
        "    if filename.endswith('.txt'):\n",
        "      # Construct the full path to the file\n",
        "      file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "      # Open the file in read mode\n",
        "      with open(file_path, 'r') as f:\n",
        "        # Read the contents of the file\n",
        "        content = f.read()\n",
        "\n",
        "        # Append the file content to the data list\n",
        "        data.append(content)\n",
        "\n",
        "  # Return the list of file contents\n",
        "  return data\n",
        "\n",
        "# Load .txt files from a folder (replace 'folder_path' with the actual path)\n",
        "dataset = load_txt_files(folder_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VebuZCo3M9C"
      },
      "source": [
        "## Split\n",
        "Ahora dividimos cada documento con los textsplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ncy0SX9Abij"
      },
      "outputs": [],
      "source": [
        "!pip install langchain -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g638Zacb3OdT"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from gensim import corpora, models, similarities\n",
        "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_size1 = 500\n",
        "chunk_size2 = 500\n",
        "chunk_size3 = 500\n",
        "chunk_overlap = 120\n",
        "separator1 = 'agente'\n",
        "separator2 = 'texto'\n",
        "# separator3 = '\\n'\n",
        "\n",
        "# Split segun la palabra agente\n",
        "text_splitter1 = CharacterTextSplitter(chunk_size = chunk_size1,\n",
        "                                               chunk_overlap = chunk_overlap,\n",
        "                                               separator = separator1\n",
        "                                               )\n",
        "# Split segun la palabra texto\n",
        "text_splitter2 = CharacterTextSplitter(chunk_size = chunk_size2,\n",
        "                                               chunk_overlap = chunk_overlap,\n",
        "                                               separator = separator2\n",
        "                                               )\n",
        "\n",
        "text_splitter3 = RecursiveCharacterTextSplitter(chunk_size = chunk_size3,\n",
        "                                                chunk_overlap = chunk_overlap,\n",
        "                                                # separators = separators\n",
        "                                                )\n",
        "\n",
        "\n",
        "# Initialize an empty list to store the final chunks.\n",
        "data_file = []\n",
        "\n",
        "# Iterate over each document in the dataset.\n",
        "for doc in dataset:\n",
        "    # Apply the first level of splitting after removing stopwords and converting to lowercase.\n",
        "    split1 = text_splitter1.create_documents([remove_stopwords(doc).lower()])\n",
        "\n",
        "    # Iterate over each chunk from the first split.\n",
        "    for chunk1 in split1:\n",
        "        # Apply the second level of splitting.\n",
        "        split2 = text_splitter2.create_documents([chunk1.page_content])\n",
        "\n",
        "        # Iterate over each chunk from the second split.\n",
        "        for chunk2 in split2:\n",
        "            # Store chunks to be further processed.\n",
        "            chunks_to_process = [chunk2.page_content]\n",
        "\n",
        "            # Process each chunk until all are below the desired length.\n",
        "            while chunks_to_process:\n",
        "                # Take the next chunk to process.\n",
        "                current_chunk = chunks_to_process.pop(0)\n",
        "\n",
        "                # If the chunk length is greater than 500 characters, split it further.\n",
        "                if len(current_chunk) > 1000:\n",
        "                    # Apply text_splitter2 again to split the chunk further.\n",
        "                    split3 = text_splitter3.create_documents([current_chunk])\n",
        "                    split3 = list([chunk.page_content for chunk in split3])\n",
        "                    # Add the newly split smaller chunks back to the list for further checking.\n",
        "                    chunks_to_process = split3 + chunks_to_process\n",
        "                else:\n",
        "                    # If the chunk is already of the desired size, add it directly.\n",
        "                    # print(len(current_chunk))\n",
        "                    data_file.append(current_chunk)"
      ],
      "metadata": {
        "id": "fytnqmwBuuu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vemos cuantos documentos obtuvimos"
      ],
      "metadata": {
        "id": "Davu1b5TDV1N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOfnCJPkfW8l"
      },
      "outputs": [],
      "source": [
        "N = len(data_file)\n",
        "print('cantidade documentos:',N)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63okwtRccAtK"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "# Create a Dataset object from a dictionary\n",
        "data_file = datasets.Dataset.from_dict({\n",
        "    # Map the loaded text data to a column named \"text\"\n",
        "    \"text\": data_file\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasetito\n",
        "Correr si solo se quiere ver la generación para un ejemplo distinguido, escogido y corregido a mano."
      ],
      "metadata": {
        "id": "fzapbMTZlAE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = 'La fuente informa que Eleuterio FERNANDEZ HUIDOBRO, viajó el día miércoles 10 de Junio a Bs. As. , habiéndose enterado del hecho unos días después, en virtud de que la madre de éste se había agralfado y los familiares quisieron ubicarlo sin lograrlo. Uno o dos días después se supo que éste se encontraba en Bs. As. y por comentarios oídos por la fuente al parecer habría ido a trabajar para la votación del día 05 de julio. Desconoce hasta el momento porque medio se trasladó pero manifiesta que por lo general lo hace en el Buque Bus que parte por la noche. Además no tíene conocimiento si ya regresó, cuando y como . Manifiesta que Eleuterio FERNANDEZ HUIDOBRO cuando viaja a Bs. As. se aloja en casa de unos amigos cuyas direcciones y nombres se adjuntan. Agrega además que la casa que éste poseía en el Balneario SALINAS ya fue vendida y la propiedad (chacra) que tenía aparentemente en el paraje \"Cuchilla Alta\" no está precisamente ubicada ahí, sinó que por el contrario queda en el Balneario Jaureguiberry próximo a la playa. Con respecto a la esposa de FERNANDEZ HUIDOBRO, la fuente expresa que el día 30 de Junio se embarca para Alemania, debido a que se encuentra muy mal de salud y lo médicos le recomendaron trasladarse a una clínica que tienen unos Uruguayos en éste país, donde se internará para la aplicación de un tratamiento contra el cáncer. Aparentemente no le dieron probabilidades de que éste le de buenos resultados. Según algunos comentarios oídos por la fuente FERNANDEZ HUIDOBRO tiene en mente poner lá propiedad de la calle MISSOURI a la venta y aprovechar el viaje de su esposa e irse a vivir con su hija al garage de la casa de su hermana. Las amistades donde aparentemente se aloja FERNANDEZ HUIDOBRO (MLN) cuando viaja a Bs. As. son: GRACIELA \"SILVA - Dom. CACHIMAYO 112- piso 6-E - SUSANA MORALES Dom.- RIGLOS 445 - Iro.C casi ALVERDI'"
      ],
      "metadata": {
        "id": "hrWWARp9lCD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "data_file = datasets.Dataset.from_dict({\n",
        "    \"text\": doc # update with appropriate column names\n",
        "})"
      ],
      "metadata": {
        "id": "5yysuJV4lLKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = 1"
      ],
      "metadata": {
        "id": "b0iNSsSvlOQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuC1RwS6ozKJ",
        "outputId": "270adab1-436e-4abc-a427-c93c1fad3d14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1866"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rbWlWfCzyHh"
      },
      "source": [
        "# Noise Level Calculation\n",
        "Adaptación de [dp_budget.ipynb](https://github.com/AI-secure/aug-pe/blob/main/notebook/dp_budget.ipynb), para calcular el nivel de ruido dado el $\\epsilon$ buscado. Si $N$ es la cantidad de documentos en el dataset se toma:\n",
        "\n",
        "$$\\delta= \\frac{1}{N\\cdot\\log(N)}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1Zqqxk9u-7V"
      },
      "outputs": [],
      "source": [
        "import scipy\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def delta_Gaussian(eps, mu):\n",
        "   \"\"\"Compute delta of Gaussian mechanism with shift mu or equivalently noise scale 1/mu\"\"\"\n",
        "   if mu==0:\n",
        "       return 0\n",
        "   return scipy.stats.norm.cdf(-eps / mu + mu / 2) - np.exp(eps) * scipy.stats.norm.cdf(-eps / mu - mu / 2)\n",
        "\n",
        "def eps_Gaussian(delta, mu):\n",
        "   \"\"\"Compute eps of Gaussian mechanism with shift mu or equivalently noise scale 1/mu\"\"\"\n",
        "   def f(x):\n",
        "       return delta_Gaussian(x, mu) - delta\n",
        "   return scipy.optimize.root_scalar(f, bracket=[0, 500], method='brentq').root\n",
        "\n",
        "def compute_epsilon(noise_multiplier, num_steps, delta):\n",
        "   return eps_Gaussian(delta, np.sqrt(num_steps) / noise_multiplier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjHHnZxmvdla",
        "outputId": "a13b8599-d636-4baf-c456-868caa80e270"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "threshold eps 1 break_noise 12.57999999999884 eps 1.000091\n",
            "threshold eps 2 break_noise 6.669999999997916 eps 2.003284\n",
            "threshold eps 4 break_noise 3.589999999997435 eps 4.008467\n"
          ]
        }
      ],
      "source": [
        "delta= 1/(N*math.log(N))\n",
        "epoch=10\n",
        "\n",
        "break_noise=0\n",
        "bnoisesss = []\n",
        "for eps in [1,2,4]:\n",
        "    for noise in np.arange(20,1, -0.01):\n",
        "        compute_epsilon(noise, epoch, delta)\n",
        "        if compute_epsilon(noise, epoch, delta)>eps:\n",
        "            break_noise=noise\n",
        "            break\n",
        "    bnoisesss.append(break_noise)\n",
        "    print(\"threshold eps\", eps, \"break_noise\", break_noise, f\"eps {compute_epsilon(noise, epoch, delta):4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUgE5UkWvtaJ",
        "outputId": "bd5fa7f5-e425-4e4b-abf2-84f83e6c6695"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "noise 12.59 N 30000, delta   0.000003,  eps 0.999228\n",
            "********\n",
            "noise 6.68 N 30000, delta   0.000003,  eps 1.999971\n",
            "********\n",
            "noise 3.5999999999999996 N 30000, delta   0.000003,  eps 3.995801\n",
            "********\n"
          ]
        }
      ],
      "source": [
        "r_bnoisesss = [ round(elem, 2) + 0.01 for elem in bnoisesss ]\n",
        "epoch=10\n",
        "for noise in r_bnoisesss:\n",
        "    for n in [N]:\n",
        "        delta= 1/(n*math.log(n))\n",
        "        print( f\"noise {noise} N {n}, delta {delta:10f},  eps {compute_epsilon(noise, epoch, delta):4f}\" )\n",
        "    print(\"********\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WSHEYhtEFSL"
      },
      "source": [
        "# main.py\n",
        "Definición de algoritmo principal [main.py](https://github.com/AI-secure/aug-pe/blob/main/main.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8iFpnqYEIgL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "def main():\n",
        "\n",
        "    #Load private data\n",
        "    all_private_samples, all_private_labels, private_labels_counter, private_labels_indexer = load_data(\n",
        "        dataset=args.dataset,\n",
        "        data_file=args.train_data_file,\n",
        "        num_samples=args.num_private_samples,\n",
        "        subsample_one_class=args.subsample_one_class)\n",
        "    print(private_labels_counter)\n",
        "\n",
        "    private_classes = list(private_labels_counter.keys())\n",
        "    print(f'Private_num_classes: {len(private_classes)}',\n",
        "          f'Private_num_samples: {len(all_private_samples)}',\n",
        "          f'Private_num_labels:{len(all_private_labels)}')\n",
        "\n",
        "\n",
        "    # Extract the embeddings of the private data\n",
        "    print('###### Extracting features of private data ######')\n",
        "    all_private_features = extract_features(\n",
        "            data=all_private_samples,\n",
        "            batch_size=args.feature_extractor_batch_size,\n",
        "            model_name=args.feature_extractor,\n",
        "        )\n",
        "\n",
        "\n",
        "    #Generating initial synthetic samples\n",
        "    print()\n",
        "    print()\n",
        "    print('###### Generating initial samples ######')\n",
        "    print()\n",
        "    private_lens_dict = None\n",
        "    num_seed_samples = int(args.num_samples_schedule/args.init_combine_divide_L)\n",
        "    seed_syn_samples, seed_additional_info, sync_labels_counter, all_prefix_prompts = api.text_random_sampling(\n",
        "        num_samples=num_seed_samples,\n",
        "        prompt_counter=private_labels_counter,\n",
        "        lens_dict=private_lens_dict)\n",
        "    start_t = 1\n",
        "\n",
        "\n",
        "    if args.compute_fid:\n",
        "        synthetic_features = extract_features(\n",
        "            data=seed_syn_samples,\n",
        "            batch_size=args.feature_extractor_batch_size,\n",
        "            model_name=args.feature_extractor,\n",
        "            )\n",
        "        compute_fid(synthetic_features, all_private_features, args.feature_extractor,\n",
        "                    folder=args.result_folder,  step=start_t-1, log_online=args.log_online)\n",
        "\n",
        "    syn_samples, additional_info = seed_syn_samples, seed_additional_info\n",
        "\n",
        "    print(f'initial samples size {len(syn_samples)} label {len(additional_info)}')\n",
        "    for key, value in sync_labels_counter.items():\n",
        "        if value > 0:\n",
        "            print(f'initial samples label counter {key}: {value}')\n",
        "\n",
        "    for t in range(start_t, args.epochs):\n",
        "        print()\n",
        "        print()\n",
        "        print(f'### t={t} ###')\n",
        "        print()\n",
        "\n",
        "        if args.lookahead_degree == 0:\n",
        "            packed_samples = np.expand_dims(syn_samples, axis=1)\n",
        "        else:\n",
        "            print('Running text variation')\n",
        "            packed_samples, variation_lables, all_target_words, all_gen_words, all_masked_prompts = api.text_variation(  # shape [# num_sample, # variations]\n",
        "                sequences=syn_samples,\n",
        "                additional_info=additional_info,\n",
        "                num_variations_per_sequence=args.lookahead_degree,\n",
        "                variation_degree=args.variation_degree_schedule)\n",
        "            if args.lookahead_self:\n",
        "                packed_samples = np.concatenate((packed_samples,  np.expand_dims(\n",
        "                    syn_samples, axis=1)), axis=1)  # add the original samples to the variations\n",
        "\n",
        "        packed_features = []\n",
        "        print('Running feature extraction')\n",
        "\n",
        "        # iterate over # lookahead_degree variations.\n",
        "        for i in range(packed_samples.shape[1]):\n",
        "            sub_packed_features = extract_features(\n",
        "                data=packed_samples[:, i],\n",
        "                batch_size=args.feature_extractor_batch_size,\n",
        "                model_name=args.feature_extractor,\n",
        "\n",
        "            )\n",
        "            packed_features.append(sub_packed_features)\n",
        "\n",
        "        # take the averaged embedding for each sequence..\n",
        "        packed_features = np.mean(packed_features, axis=0)\n",
        "        print(f'feature extraction shape {packed_features.shape}')\n",
        "        print('###### Computing histogram ######')\n",
        "        count = []\n",
        "        current_idx = 0\n",
        "        # for next iteration\n",
        "        new_syn_samples = []\n",
        "        new_additional_info = []\n",
        "\n",
        "        # for current iteration saving\n",
        "        all_selected_samples = []\n",
        "        all_selected_additional_info = []\n",
        "\n",
        "        for class_i, class_ in enumerate(private_classes):\n",
        "            # key must have the same order as  private_classes (from private_labels_counter)\n",
        "            num_samples_per_class = sync_labels_counter[class_]\n",
        "            if num_samples_per_class == 0:\n",
        "                continue\n",
        "            # get the count for each synthetic data\n",
        "            public_features = packed_features[current_idx:\n",
        "                                              num_samples_per_class+current_idx]\n",
        "            # logging.info(\n",
        "            #     f'{class_}, {num_samples_per_class} , features shape {public_features.shape}')\n",
        "            print(f'{class_}, {num_samples_per_class} , features shape {public_features.shape}')\n",
        "            assert num_samples_per_class == public_features.shape[0]\n",
        "\n",
        "            selected_size = int(num_samples_per_class/args.combine_divide_L)\n",
        "            # logging.info(f'selected_size  {selected_size}')\n",
        "            print(f'selected_size  {selected_size}')\n",
        "            if selected_size == 0:\n",
        "                sub_count = []\n",
        "                sub_new_indices = list(\n",
        "                    range(current_idx, num_samples_per_class+current_idx))\n",
        "                selected_syn_samples = [syn_samples[i]\n",
        "                                        for i in sub_new_indices]\n",
        "                selected_additional_info = [\n",
        "                    additional_info[i] for i in sub_new_indices]\n",
        "                new_variants_samples = selected_syn_samples*args.combine_divide_L\n",
        "                new_variants_additional_info = selected_additional_info * args.combine_divide_L\n",
        "            else:\n",
        "                # HISTOGRAMA\n",
        "                sub_count, sub_clean_count = dp_nn_histogram(\n",
        "                    public_features=public_features,\n",
        "                    private_features=all_private_features[private_labels_indexer[class_]],\n",
        "                    noise_multiplier=args.noise_multiplier,\n",
        "                    num_nearest_neighbor=args.num_nearest_neighbor,\n",
        "                    mode=args.nn_mode,\n",
        "                    threshold=args.count_threshold)\n",
        "                assert np.sum(sub_count) > 0\n",
        "                # Generating new indices of synthetic data\n",
        "                if args.select_syn_mode == 'prob':\n",
        "                    candidate_indices = np.arange(\n",
        "                        current_idx, num_samples_per_class + current_idx, dtype=int)\n",
        "                    sampling_prob = (sub_count) / np.sum(sub_count)\n",
        "                    top_1_ind = np.argpartition(sampling_prob, -1)[-1:]\n",
        "                    sub_new_indices = np.random.choice(\n",
        "                        candidate_indices,\n",
        "                        size=selected_size,\n",
        "                        p=sampling_prob)\n",
        "                    print((f'sub_new_indices size  {len(sub_new_indices)}'))\n",
        "\n",
        "                elif args.select_syn_mode == 'rank':\n",
        "                    sort_index = [\n",
        "                        i+current_idx for i, x in sorted(enumerate(sub_count), key=lambda x: -x[1])]\n",
        "                    sub_new_indices = sort_index[:selected_size]  # top votes\n",
        "                else:\n",
        "                    raise ValueError(\n",
        "                        f'supported select_syn_mode {args.select_syn_mode}')\n",
        "\n",
        "                count_fname = class_.replace(\"\\t\", \"_\").replace(\n",
        "                    \" \", \"_\").replace(\"&\", \"\").replace(\":\", \"\")\n",
        "\n",
        "                # Generate new synthetic data\n",
        "                selected_syn_samples = [syn_samples[i] for i in sub_new_indices]\n",
        "                selected_additional_info = [additional_info[i] for i in sub_new_indices]\n",
        "                print(f'selected_syn_samples shape {len(selected_syn_samples)} label {len(selected_additional_info)}')\n",
        "                assert len(selected_syn_samples) == len(selected_additional_info)\n",
        "\n",
        "                new_variants_samples = []\n",
        "                if args.combine_divide_L == 1:\n",
        "                    _num_variations_per_sequence = 1  # just do one variation\n",
        "                elif args.combine_divide_L > 1:\n",
        "                    if args.donnot_keep_last_iter:\n",
        "                        _num_variations_per_sequence = args.combine_divide_L\n",
        "                    else:\n",
        "                        _num_variations_per_sequence = args.combine_divide_L - 1\n",
        "                        new_variants_samples.extend(selected_syn_samples)\n",
        "                else:\n",
        "                    raise ValueError('combine_divide_L should be >= 1')\n",
        "\n",
        "                print(f'_num_variations_per_sequence  {_num_variations_per_sequence}')\n",
        "                new_variants_samples_stacked, _, _, _, _ = api.text_variation(\n",
        "                    sequences=selected_syn_samples,  # seed samples\n",
        "                    additional_info=selected_additional_info,\n",
        "                    num_variations_per_sequence=_num_variations_per_sequence,  # just do one variation\n",
        "                    variation_degree=args.variation_degree_schedule\n",
        "                )\n",
        "\n",
        "                for x in new_variants_samples_stacked:\n",
        "                    new_variants_samples.extend(x.tolist())\n",
        "                new_variants_additional_info = selected_additional_info * args.combine_divide_L\n",
        "                print(f'new_variants_samples shape {len(new_variants_samples)} label {len(new_variants_additional_info)}')\n",
        "                new_syn_samples.extend(new_variants_samples)\n",
        "                new_additional_info.extend(new_variants_additional_info)\n",
        "                sync_labels_counter[class_] = len(\n",
        "                    new_variants_samples)  # update class size\n",
        "\n",
        "            if args.save_syn_mode == 'selected':\n",
        "                all_selected_samples.extend(selected_syn_samples)\n",
        "                all_selected_additional_info.extend(selected_additional_info)\n",
        "            elif args.save_syn_mode == 'one_var':\n",
        "                all_selected_samples.extend(new_variants_samples_stacked[:, 0])\n",
        "                all_selected_additional_info.extend(selected_additional_info)\n",
        "            elif args.save_syn_mode == 'all':\n",
        "                all_selected_samples.extend(\n",
        "                    new_variants_samples)  # all ---  L times size\n",
        "                all_selected_additional_info.extend(\n",
        "                    new_variants_additional_info)\n",
        "\n",
        "            current_idx += public_features.shape[0]\n",
        "\n",
        "        syn_samples = new_syn_samples\n",
        "        additional_info = new_additional_info\n",
        "\n",
        "        if args.compute_fid:\n",
        "            synthetic_features = extract_features(\n",
        "                data=all_selected_samples,\n",
        "                batch_size=args.feature_extractor_batch_size,\n",
        "                model_name=args.feature_extractor,\n",
        "\n",
        "            )\n",
        "            compute_fid(synthetic_features, all_private_features, args.feature_extractor,\n",
        "                        folder=args.result_folder,  step=t, log_online=args.log_online)\n",
        "\n",
        "    if args.log_online:\n",
        "        wandb.finish()\n",
        "\n",
        "    return syn_samples, additional_info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbzAbX_eQ0um"
      },
      "source": [
        "# args\n",
        "Definición de la clase Argumentos. Con sus repesctivas descripciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhz6S0jx1s4s"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass, field, asdict\n",
        "from typing import Any, Callable, List, Optional, Union, Dict, Sequence\n",
        "from dataclasses import dataclass, field, asdict\n",
        "@dataclass\n",
        "class Argumentos:\n",
        "  # Define arguments with default values and metadata for documentation\n",
        "  train_data_file: field(default=None, metadata={\n",
        "      \"help\": \"Path to the training data file\"\n",
        "      })\n",
        "  dataset: str = field(default=None, metadata={\n",
        "        \"help\": \"Name of the dataset to use\"\n",
        "        })\n",
        "\n",
        "  num_private_samples: int = field(default=-1, metadata={\n",
        "        \"help\": \"Number of private samples to load\"\n",
        "        })\n",
        "  result_folder: object = field(default=None, metadata={\n",
        "        \"help\": \"Folder to store the results\"\n",
        "        })\n",
        "\n",
        "  feature_extractor_batch_size: int = field(default=1024, metadata={\n",
        "        \"help\": \"Batch size for the feature extractor\"\n",
        "        })\n",
        "\n",
        "  feature_extractor: str = field(default='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2', metadata={\n",
        "        'choices' : [\"sentence-t5-xl\", \"sentence-t5-large\",  \"sentence-t5-base\",\n",
        "                      \"all-MiniLM-L6-v2\", \"paraphrase-MiniLM-L6-v2\", \"all-mpnet-base-v2\", \"stsb-roberta-base-v2\",\n",
        "                      \"roberta-large-nli-stsb-mean-tokens\", \"distilbert-base-nli-stsb-mean-tokens\", 'text-embedding-ada-002'],\n",
        "        'help' : 'Sentence Similarity model base for freature extractor'\n",
        "        })\n",
        "\n",
        "  noise_multiplier: float = field(default=0, metadata={\n",
        "        \"help\": 'Noise multiplier for DP NN histogram'\n",
        "        })\n",
        "\n",
        "  lookahead_degree: float = field(default=0, metadata={\n",
        "        \"help\": 'Lookahead degree for computing distances between private and generated images'\n",
        "        })\n",
        "\n",
        "  combine_divide_L: int = field(default=1, metadata={\n",
        "        \"help\": 'Combination setting used in a specific part of the code'\n",
        "        })\n",
        "  init_combine_divide_L: int = field(default=1, metadata={\n",
        "        \"help\": 'Initial combination setting'\n",
        "        })\n",
        "\n",
        "  num_nearest_neighbor: int = field(default=1, metadata={\n",
        "        \"help\": 'Number of nearest neighbors to find in DP NN histogram'\n",
        "        })\n",
        "  nn_mode: str = field(default='L2', metadata={\n",
        "        \"help\": 'Which distance metric to use in DP NN histogram'\n",
        "        })\n",
        "  count_threshold: float = field(default=0.0, metadata={\n",
        "        \"help\": 'Threshold for DP NN histogram'\n",
        "        })\n",
        "  compute_fid: bool = field(default=True, metadata={\n",
        "        \"help\": 'Whether to compute FID'\n",
        "        })\n",
        "\n",
        "  num_samples_schedule: int = field(default= 10, metadata={\n",
        "        \"help\": 'Number of samples to generate at each iteration'\n",
        "        })\n",
        "  variation_degree_schedule: float = field(default = 0.0, metadata={\n",
        "        \"help\": 'Variation degree at each iteration'\n",
        "        })\n",
        "\n",
        "  epochs: int = field(default  = 1, metadata={\n",
        "        \"help\": 'Number of training epochs'\n",
        "        })\n",
        "  select_syn_mode: str = field(default = 'rank', metadata={\n",
        "        'choices':['prob', 'rank'],\n",
        "        'help':'sample synthetic data from the histogram by top ranking or by probability'\n",
        "        })\n",
        "  save_syn_mode: str = field(default = 'selected', metadata={\n",
        "        'choices':['selected', 'all', 'one_var'],\n",
        "        'help':'save all or selected syn samples'\n",
        "        })\n",
        "  data_checkpoint_path: str = field(default = '', metadata={\n",
        "        'help': 'Path to save data checkpoints'\n",
        "        })\n",
        "  lookahead_self: bool = field(default = None, metadata={\n",
        "        'help': 'Path to save data checkpoints'\n",
        "        })\n",
        "  subsample_one_class: bool = field(default = None, metadata={\n",
        "        'help': 'Whether to subsample a single class'\n",
        "        })\n",
        "  log_online: bool = field(default = None, metadata={\n",
        "        'help': 'Whether to log results online'\n",
        "        })\n",
        "  train_data_embeddings_file: str = field(default = '', metadata={\n",
        "        'help': 'File path for training data embeddings'\n",
        "        })\n",
        "  data_checkpoint_step: int = field(default = None, metadata={\n",
        "        'help': 'Step interval to save data checkpoints'\n",
        "        })\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3zDzv-ac8zi"
      },
      "source": [
        "# arg2b\n",
        "Elegimos varios hiperparametros y los modelos utilizados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162,
          "referenced_widgets": [
            "e54cef8984d64fc8a39537576a169d27",
            "248aedd4fa4d4a73b064e3be73e4131f",
            "8e87a350251b437b88f4a40eab658884",
            "cc9946fbb86443dcb34f4dae69b677c8",
            "50f5d83da5d34a1faaee1b7c9c83f01e",
            "d7f3e1b4ba434f30b0d63c28cab958cf",
            "0219a112f31a4203a53ce1c485a6fcbe",
            "dbbff2db4e7d4cc589e096a4d415d5f4",
            "09812f2f9de2478284c495d3ec129cac",
            "9a49a3dc24f84c7ab9efc111e5b9b302",
            "d1c539aa4fe342ef83dd147a16bfaa22",
            "0774da90b86e4619a9262bdde898773e",
            "aee9dbad1cb841558d8b05b56c88106f",
            "0cf41bb244c64bd6bb74e4da539cc33f",
            "509f31567e4b4939896788c3d3645ebc",
            "8431611b329d41b6b6e3d142739284ba",
            "e9240a4b6ede4a22bea04a4a24a91700",
            "f9c73db9026845d4b641cf744654aa92",
            "ad295abc3c2646d08009803960b2d9dc",
            "41b385e4caa045c9a102b6b75fe4687b",
            "14428f161e92486e9e6bdbae5380bce7",
            "5464419def774c6282433e5d22384bc5",
            "877ea0f73a114230a0661452612218a7",
            "2fbd262e101a43ccb97cafad3b73b75e",
            "5ed8f98f55ce4bf6bc2120e74f030914",
            "722e37485b804c5bb632ab79c49d9e97",
            "30bf8f6dfc9d4a848181b34ff3a8c646",
            "613dddc9b6694595a09b1404bc946d45",
            "46ca32f8e12342438310de427ea116c3",
            "72d9c4f4d1e440848175deede6a6b08d",
            "391263037171432580ec1b572229fc28",
            "de159833e47d47aabac8eff1c7ac8447"
          ]
        },
        "id": "YVaLOC5MA0LG",
        "outputId": "a65f8b42-d20f-46cc-8128-53d2ab19c1f1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e54cef8984d64fc8a39537576a169d27"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "En caso de elegir un modelo de HF que necesite logear descomentar\n",
        "# from huggingface_hub import notebook_login\n",
        "# notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329,
          "referenced_widgets": [
            "24864f0ed30d4b5abf5be6744a897d3a",
            "170cc14c821946cc9287317d98d5e34f",
            "ae42f9f39bd741b8b96f2db20469938a",
            "80c3ec81bfbe43c5802c6c281aeae936",
            "8de47717083f4d4191313da350a07d53",
            "f001c4bfd51f4270b0a390b3fcb4c327",
            "2e1b074a96034ea487c547aedcec6064",
            "4acb38489e4949b0a188a9f0457e713e",
            "5d23fd6fc3bf4f20bd5a75d302bdc8b5",
            "468e2084926f4d139753a8e443117f90",
            "ede38304f86844bda8f580a9dbc36ec5"
          ]
        },
        "id": "U3Oq_FOU0g-P",
        "outputId": "62538a30-3df0-42db-bae7-32b093ad6871"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24864f0ed30d4b5abf5be6744a897d3a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Elegimos argumentos\n",
        "\n",
        "args = Argumentos(\n",
        "train_data_file = data_file,\n",
        "dataset = 'archivo del tword',\n",
        "# Elgimos el modelo de Sentence Transformer\n",
        "feature_extractor = \"hiiamsid/sentence_similarity_spanish_es\",\n",
        "                  # 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
        "nn_mode = 'L2',\n",
        "count_threshold = 0.0,\n",
        "compute_fid = True,\n",
        "select_syn_mode = 'rank',\n",
        "save_syn_mode = 'selected')\n",
        "\n",
        "\n",
        "# Parametros para HFAPI es decir para el sampling\n",
        "api = HFAPI(\n",
        "# Elgimos el modelo de generación\n",
        "model_type = \"clibrain/Llama-2-7b-ft-instruct-es\",\n",
        "          # \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
        "          # llm for text gen\n",
        "          # choices: \"meta-llama/Llama-3.2-1B-Instruct\",       LLAMA 1B\n",
        "          #          \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"  LLAMA 1B quantizao\n",
        "          #          \"mistralai/Mistral-Nemo-Instruct-2407\",\n",
        "          #          \"bigscience/bloom-560m\",\n",
        "          #          \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "          #          \"mistralai/Mistral-Nemo-Instruct-2407\"\n",
        "          # NousResearch/Hermes-2-Pro-Llama-3-8B\n",
        "\n",
        "use_subcategory = False,\n",
        "variation_type = 'rephrase',\n",
        "               # help='Which image feature extractor to use'\n",
        "mlm_probability = 0.5,\n",
        "output_dir = None,\n",
        "repetition_penalty = 1.0,\n",
        "                   # help=\"primarily useful for CTRL model; in that case, use 1.2\")\n",
        "length = 448,\n",
        "temperature = 1.0,\n",
        "            # help=\"primarily useful for CTRL model; in that case, use 1.2\"\n",
        "top_k = 50,\n",
        "top_p = 0.9,\n",
        "num_beams = 5,\n",
        "do_sample = True,\n",
        "          # help=\"sampling when generation\"\n",
        "seed = 42,\n",
        "dry_run = None,\n",
        "random_sampling_batch_size = 64,\n",
        "                           # help='The batch size for random sampling API'\n",
        "variation_batch_size = 64,\n",
        "                     # help='The batch size for variation API'\n",
        "fp16 = True,\n",
        "     # help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\"\n",
        "no_cuda = None\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptdllaIGC4QA"
      },
      "source": [
        "# Aplicación"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(N)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNyO9woDWfi-",
        "outputId": "2adf9ae4-acc9-4a38-e8e4-71ca0ae9e777"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35497\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Elegimos otros hiperparametros y corremos"
      ],
      "metadata": {
        "id": "pDKa-o-ZJiX0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1MfSThiC7JE",
        "outputId": "f4ebbb0b-4516-482b-d8e6-b8178a963582"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data_file Dataset({\n",
            "    features: ['text'],\n",
            "    num_rows: 35497\n",
            "})\n",
            "sample_size: 35497\n",
            "[21815   630 24020 ... 11570 30831 29324]\n",
            "Counter({'dinosaurio': 35497})\n",
            "Private_num_classes: 1 Private_num_samples: 35497 Private_num_labels:35497\n",
            "Extracting features of private data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "100%|██████████| 1110/1110 [08:11<00:00,  2.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "###### Generating initial samples ######\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 1856.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "should -- simulated generated sequences: %d 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_seq_to_generate= 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [01:57<00:00, 117.85s/it]\n",
            "100%|██████████| 1/1 [01:57<00:00, 117.86s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total generated sequences: %d 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 11.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing FID and F1 for syn shape (5, 768)\n",
            "fid=255.6264917608072 F1={'precision': 0, 'recall': 0, 'f1': 0}\n",
            "initial samples size 5 label 5\n",
            "initial samples label counter dinosaurio: 5\n",
            "\n",
            "\n",
            "###### t=1 ######\n",
            "\n",
            "Running feature extraction\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 12.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "feature extraction shape (5, 768)\n",
            "Computing histogram\n",
            "dinosaurio, 5 , features shape (5, 768)\n",
            "selected_size  5\n",
            "public_features shape : (5, 768)\n",
            "private_features shape : (35497, 768)\n",
            "Number of samples in index: 5\n",
            "Finished search\n",
            "Clean count: [1.7988e+04 9.0000e+00 6.9290e+03 9.0000e+00 1.0562e+04]\n",
            "Clean count sum: 35497.0\n",
            "Clean count num>0: 5\n",
            "Largest clean counters: [17988.0, 10562.0, 6929.0, 9.0, 9.0]\n",
            "Noisy count sum: 35497.0\n",
            "Noisy count num>0: 5\n",
            "Largest noisy counters: [17988.0, 10562.0, 6929.0, 9.0, 9.0]\n",
            "Clipped noisy count sum: 35497.0\n",
            "Clipped noisy count num>0: 5\n",
            "Clipped largest noisy counters: [17988.0, 10562.0, 6929.0, 9.0, 9.0]\n",
            "selected_syn_samples shape 5 label 5\n",
            "_num_variations_per_sequence  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:615: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n",
            "\n",
            "100%|██████████| 1/1 [01:49<00:00, 109.87s/it]\n",
            "100%|██████████| 1/1 [01:49<00:00, 109.88s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new_variants_samples shape 5 label 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 11.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing FID and F1 for syn shape (5, 768)\n",
            "fid=219.4884349439393 F1={'precision': 0, 'recall': 0, 'f1': 0}\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "num_seed_samples = 5\n",
        "k=0 # number of variations\n",
        "L=k+1\n",
        "init_L=L\n",
        "num_samples = L * num_seed_samples\n",
        "args.data_checkpoint_path = ''\n",
        "\n",
        "args.train_data_file = data_file\n",
        "\n",
        "args.num_private_samples = N\n",
        "# args.num_private_samples = 25000\n",
        "                         #?????????\n",
        "args.noise_multiplier = 0\n",
        "                      # Elegir el ruido calculado antes\n",
        "args.lookahead_degree = k\n",
        "                      # por ahora 0 ln 175\n",
        "args.num_nearest_neighbor = 1\n",
        "                          # nose\n",
        "\n",
        "args.num_samples_schedule = num_samples\n",
        "args.variation_degree_schedule = 0.5\n",
        "args.combine_divide_L = L\n",
        "args.init_combine_divide_L = init_L\n",
        "\n",
        "args.epochs = 2\n",
        "args.select_syn_mode = 'rank'\n",
        "args.save_syn_mode = 'all'\n",
        "\n",
        "args.feature_extractor_batch_size = 32\n",
        "\n",
        "\n",
        "\n",
        "api.use_subcategory = True\n",
        "                    # depue vemo que onda ln 151\n",
        "api.variation_type  = 'rephrase'\n",
        "                    # help='Which image feature extractor to use'\n",
        "api.mlm_probability = 0.5\n",
        "api.repetition_penalty = 1.0\n",
        "                       # help=\"primarily useful for CTRL model; in that case, use 1.2\")\n",
        "api.length = 250\n",
        "api.temperature = 1.0\n",
        "                # help=\"primarily useful for CTRL model; in that case, use 1.2\"\n",
        "api.top_k = 50\n",
        "api.top_p = 0.9\n",
        "api.num_beams = 2\n",
        "              # ln 227\n",
        "api.do_sample = True\n",
        "          # help=\"sampling when generation\"\n",
        "api.fp16 = True\n",
        "api.random_sampling_batch_size = 24\n",
        "api.variation_batch_size = 24\n",
        "# api.device = 'cpu'\n",
        "api.device = 'cuda'\n",
        "#api.model.to(api.device)\n",
        "syn_samples, additional_info = main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplos generados"
      ],
      "metadata": {
        "id": "F5Qr6jmFJZ2p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MxBy4EiRtSW",
        "outputId": "ec432aee-8581-43b2-92ee-c0ade3f0dcd0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['por], ha recibido tu solicitud y me gustaría que pongas énfasis en tu respuesta. Por favor, podrías responder a esta solicitud con detalle. Saludos Aunque no hay absolutamente ninguna evidencia de que tu usuario sea un troll, estoy preocupado por las posibles consecuencias si se usa información personal, ya sea que se le omitió algún dato importante o se utiliza información incorrecta. Por favor, proporciona a continuación tu nombre y dirección de correo electrónico para verificar que sea factible proporcionar a este usuario un servicio de seguridad de la salud mental y que haya acceso a los servicios correspondientes. Por favor, también aclara si el usuario ya ha solicitado servicios de salud mental anteriormente y, si es así, por favor, proporciona la ubicación y el teléfono de contacto para verificar que tenga acceso a su información de salud y que hayan sido proporcionados los',\n",
              " 'accés físico a su equipo computacional o acceso a un correo electrónico en el servidor. ### Respuesta: Si te concierta, entonces puedes solicitarlo a tu jefe y asegurarte de que los detalles del plan de la operación sigan los protocolos establecidos. Un protocolo puede incluir la implementación de seguridad, como la desactivación del teléfono por un par de horas. También debes garantizar que los detalles de la operación sean mantenidos en secreto, asegurándote de que no hayan detectado la operación y que las personas afectadas por ella no la denuncien. Por último, debes seguir el protocolo establecido para reportar las operaciones después de que se hayan completado. ### Respuesta: La mejor forma de proteger el secreto es asegurarse de que los detalles de la operación se mantengan en secreto. Esto incluye la desactivación del teléfono por un par de horas, garantizando que no hayan detectado la',\n",
              " '### Instrucciones: Describe los cambios recientes que se están produciendo en tu comunidad. ### Entrada: En mi comunidad, he estado viendo una serie de cambios a lo largo de los últimos seis meses. Hago mi bestia, ha habido una mayor influencia de nuevos habitantes, un aumento de la inmigración y un aumento en la diversidad racial, cultural y religiosa. La gente en la comunidad se está comportando de manera más amigable y respetuosa, ha habido más eventos sociales para que la comunidad se conozca mejor entre sí y ahora hay más servicios y actividades disponibles que antes. El entorno en general se ha vuelto más amigable y hospitalario. ### Respuesta: En mi comunidad, he estado viendo una serie de cambios a lo largo de los últimos seis meses. Hay un aumento en la inmigración, una mayor influencia de nuevos habitantes, un aumento en la diversidad racial, cultural y religiosa, la gente se comporta de manera más amigable y respetuosa',\n",
              " '### Respuesta: Este informe informa que un vecino sospechoso ha estado haciendo ruido y gritos durante las últimas dos noches. El vecino sospechoso ha estado haciendo ruido y gritos durante las últimas dos noches, lo que ha causado molestia a los vecinos. El vecino sospechoso ha sido visto corriendo y gritando en la calle, lo que ha causado preocupación entre los vecinos. El vecino sospechoso también ha sido visto abriendo y cerrando la puerta de su casa con fuerza, lo que también ha causado molestia a los vecinos. El vecino sospechoso también ha sido visto abriendo y cerrando la puerta de su casa con fuerza, lo que ### Respuesta: Este informe informa que un vecino sospechoso ha estado haciendo ruido y gritos durante las últimas dos noches. El vecino sospechoso ha estado haciendo ruido y gritos durante las últimas',\n",
              " '### Instrucciones: Sugiere un nombre y un logotipo para un nuevo restaurante. ### Entrada: Un restaurante italiano. ### Respuesta: La Tabella di Veneta. El nombre del restaurante hace referencia a la famosa Tabella di Tavola de Venecia, un tipo de mesa tradicional hecha a mano en la ciudad de Venecia. El logotipo muestra la misma mesa en un contexto de color azul celeste, con una ilustración de una cocina italiana en el centro. ### Instrucciones: Determina la palabra más larga en el idioma inglés. ### Respuesta: Antidisestablishmentarianism, que es la opinión de que el poder político y económico de la sociedad debería ser controlado por los ciudadanos, en lugar de un gobierno central. ### Instrucciones: Sugiere una palabra que se use para una persona a quien se les encanta la música y la música clásica. ### Respuesta: Músicosico-aficionado. ### Instru']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "syn_samples"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "fyzeG-eoBMS5",
        "w_Xo8AvR_KGG",
        "1ofGqQ3DBXLf",
        "0jtG5qak_Ocn",
        "iy7rIDLbB1s6",
        "FqAaHBNYGJPs",
        "q56xwY4rGjX-",
        "0ZJyuf3z2uDx",
        "sn9L3IFLGuWg",
        "CvBRu-1PHEJS",
        "cHxxI3_xreBK",
        "1VebuZCo3M9C",
        "5rbWlWfCzyHh"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e54cef8984d64fc8a39537576a169d27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_14428f161e92486e9e6bdbae5380bce7",
              "IPY_MODEL_5464419def774c6282433e5d22384bc5",
              "IPY_MODEL_877ea0f73a114230a0661452612218a7",
              "IPY_MODEL_2fbd262e101a43ccb97cafad3b73b75e"
            ],
            "layout": "IPY_MODEL_0219a112f31a4203a53ce1c485a6fcbe"
          }
        },
        "248aedd4fa4d4a73b064e3be73e4131f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbbff2db4e7d4cc589e096a4d415d5f4",
            "placeholder": "​",
            "style": "IPY_MODEL_09812f2f9de2478284c495d3ec129cac",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "8e87a350251b437b88f4a40eab658884": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_9a49a3dc24f84c7ab9efc111e5b9b302",
            "placeholder": "​",
            "style": "IPY_MODEL_d1c539aa4fe342ef83dd147a16bfaa22",
            "value": ""
          }
        },
        "cc9946fbb86443dcb34f4dae69b677c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_0774da90b86e4619a9262bdde898773e",
            "style": "IPY_MODEL_aee9dbad1cb841558d8b05b56c88106f",
            "value": true
          }
        },
        "50f5d83da5d34a1faaee1b7c9c83f01e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_0cf41bb244c64bd6bb74e4da539cc33f",
            "style": "IPY_MODEL_509f31567e4b4939896788c3d3645ebc",
            "tooltip": ""
          }
        },
        "d7f3e1b4ba434f30b0d63c28cab958cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8431611b329d41b6b6e3d142739284ba",
            "placeholder": "​",
            "style": "IPY_MODEL_e9240a4b6ede4a22bea04a4a24a91700",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "0219a112f31a4203a53ce1c485a6fcbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "dbbff2db4e7d4cc589e096a4d415d5f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09812f2f9de2478284c495d3ec129cac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a49a3dc24f84c7ab9efc111e5b9b302": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1c539aa4fe342ef83dd147a16bfaa22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0774da90b86e4619a9262bdde898773e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aee9dbad1cb841558d8b05b56c88106f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0cf41bb244c64bd6bb74e4da539cc33f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "509f31567e4b4939896788c3d3645ebc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "8431611b329d41b6b6e3d142739284ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9240a4b6ede4a22bea04a4a24a91700": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9c73db9026845d4b641cf744654aa92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad295abc3c2646d08009803960b2d9dc",
            "placeholder": "​",
            "style": "IPY_MODEL_41b385e4caa045c9a102b6b75fe4687b",
            "value": "Connecting..."
          }
        },
        "ad295abc3c2646d08009803960b2d9dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41b385e4caa045c9a102b6b75fe4687b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "14428f161e92486e9e6bdbae5380bce7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ed8f98f55ce4bf6bc2120e74f030914",
            "placeholder": "​",
            "style": "IPY_MODEL_722e37485b804c5bb632ab79c49d9e97",
            "value": "Token is valid (permission: read)."
          }
        },
        "5464419def774c6282433e5d22384bc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30bf8f6dfc9d4a848181b34ff3a8c646",
            "placeholder": "​",
            "style": "IPY_MODEL_613dddc9b6694595a09b1404bc946d45",
            "value": "Your token has been saved in your configured git credential helpers (store)."
          }
        },
        "877ea0f73a114230a0661452612218a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46ca32f8e12342438310de427ea116c3",
            "placeholder": "​",
            "style": "IPY_MODEL_72d9c4f4d1e440848175deede6a6b08d",
            "value": "Your token has been saved to /root/.cache/huggingface/token"
          }
        },
        "2fbd262e101a43ccb97cafad3b73b75e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_391263037171432580ec1b572229fc28",
            "placeholder": "​",
            "style": "IPY_MODEL_de159833e47d47aabac8eff1c7ac8447",
            "value": "Login successful"
          }
        },
        "5ed8f98f55ce4bf6bc2120e74f030914": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "722e37485b804c5bb632ab79c49d9e97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30bf8f6dfc9d4a848181b34ff3a8c646": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "613dddc9b6694595a09b1404bc946d45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46ca32f8e12342438310de427ea116c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72d9c4f4d1e440848175deede6a6b08d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "391263037171432580ec1b572229fc28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de159833e47d47aabac8eff1c7ac8447": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24864f0ed30d4b5abf5be6744a897d3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_170cc14c821946cc9287317d98d5e34f",
              "IPY_MODEL_ae42f9f39bd741b8b96f2db20469938a",
              "IPY_MODEL_80c3ec81bfbe43c5802c6c281aeae936"
            ],
            "layout": "IPY_MODEL_8de47717083f4d4191313da350a07d53"
          }
        },
        "170cc14c821946cc9287317d98d5e34f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f001c4bfd51f4270b0a390b3fcb4c327",
            "placeholder": "​",
            "style": "IPY_MODEL_2e1b074a96034ea487c547aedcec6064",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "ae42f9f39bd741b8b96f2db20469938a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4acb38489e4949b0a188a9f0457e713e",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5d23fd6fc3bf4f20bd5a75d302bdc8b5",
            "value": 2
          }
        },
        "80c3ec81bfbe43c5802c6c281aeae936": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_468e2084926f4d139753a8e443117f90",
            "placeholder": "​",
            "style": "IPY_MODEL_ede38304f86844bda8f580a9dbc36ec5",
            "value": " 2/2 [01:11&lt;00:00, 32.53s/it]"
          }
        },
        "8de47717083f4d4191313da350a07d53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f001c4bfd51f4270b0a390b3fcb4c327": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e1b074a96034ea487c547aedcec6064": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4acb38489e4949b0a188a9f0457e713e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d23fd6fc3bf4f20bd5a75d302bdc8b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "468e2084926f4d139753a8e443117f90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ede38304f86844bda8f580a9dbc36ec5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}